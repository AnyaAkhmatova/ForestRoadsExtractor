{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom torchvision.io import read_image\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom IPython.display import clear_output","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_transform(image):\n    image = (image.float() / 255.0 - 0.5) / 0.5\n    return image\n\ndef mask_transform(mask):\n    new_mask = torch.zeros(mask[0].shape, dtype=torch.int64)\n    new_mask[torch.logical_and(torch.logical_and(mask[0, :, :] >= 200, mask[1, :, :] >= 200), mask[2, :, :] >= 200)] = 1\n    mask = new_mask\n    return mask\n\ntrain_image_transforms = torch.nn.Sequential(\n    transforms.Resize(512, antialias=True)\n)\n\nval_image_transforms = torch.nn.Sequential(\n    transforms.Resize(512, antialias=True)\n)\n\nmask_transforms = torch.nn.Sequential(\n    transforms.Resize(512, antialias=True)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ForestRoadsDataset(Dataset):\n\n    def __init__(self, csv_file, root_dir, img_transform=None, m_transform=None, max_len=None):\n        \n        self.image_mask_df = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.img_transform = img_transform\n        self.m_transform = m_transform\n        self.max_len = len(self.image_mask_df)\n        if max_len != None:\n            self.max_len = max_len\n\n    def __len__(self):\n        return self.max_len\n\n    def __getitem__(self, index):\n\n        img_name = os.path.join(self.root_dir,\n                                self.image_mask_df.iloc[index, 0])\n        m_name = os.path.join(self.root_dir,\n                                self.image_mask_df.iloc[index, 1])\n        \n        image = read_image(img_name)\n        if self.img_transform:\n            image = self.img_transform(image)\n            image = image_transform(image)\n            \n        mask = read_image(m_name)\n        if self.m_transform:\n            mask = self.m_transform(mask)\n            mask = mask_transform(mask)\n            \n        return image, mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ForestRoadsDataset('/kaggle/input/coursework/data/train/train_info.csv', \n                                   '/kaggle/input/coursework/data/train', \n                                   train_image_transforms, mask_transforms, 8000)\nval_dataset = ForestRoadsDataset('/kaggle/input/coursework/data/validation/validation_info.csv', \n                                  '/kaggle/input/coursework/data/validation', \n                                  val_image_transforms, mask_transforms)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, pin_memory=True, num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_losses(train_losses, val_losses, train_accuracies, val_accuracies, recalls=None, precisions=None, qs=None, f1s=None):\n    clear_output()\n    fig, axs = plt.subplots(2, 2, figsize=(13, 8))\n    axs[0, 0].plot(range(1, len(train_losses) + 1), train_losses, label='train')\n    axs[0, 0].plot(range(1, len(val_losses) + 1), val_losses, label='validation')\n    axs[0, 0].set_ylabel('loss')\n    axs[0, 0].set_xlabel('epoch')\n    axs[0, 0].legend()\n\n    axs[0, 1].plot(range(1, len(train_accuracies) + 1), train_accuracies, label='train')\n    axs[0, 1].plot(range(1, len(val_accuracies) + 1), val_accuracies, label='validation')\n    axs[0, 1].set_ylabel('accuracy')\n    axs[0, 1].set_xlabel('epoch')\n    axs[0, 1].legend()\n    \n    \n    if recalls is not None:\n        axs[1, 0].plot(range(1, len(recalls) + 1), recalls, label='recall')\n    if precisions is not None:\n        axs[1, 0].plot(range(1, len(precisions) + 1), precisions, label='precision')\n    if qs is not None:\n        axs[1, 0].plot(range(1, len(qs) + 1), qs, label='iou')\n    if f1s is not None:\n        axs[1, 0].plot(range(1, len(f1s) + 1), f1s, label='f1')\n    axs[1, 0].set_ylabel('validation metrics')\n    axs[1, 0].set_xlabel('epoch')\n    axs[1, 0].legend()\n    \n    plt.show()\n\ndef training_epoch(model, optimizer, criterion, train_loader, tqdm_desc):\n    train_loss, train_accuracy = 0.0, 0.0\n    model.train()\n    for images, masks in tqdm(train_loader, desc=tqdm_desc):\n        images = images.to(device)\n        masks = masks.to(device)\n\n        optimizer.zero_grad()\n        logits = model(images)['out']\n        loss = criterion(logits, masks)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() * images.shape[0]\n        train_accuracy += (logits.argmax(dim=1) == masks).sum().item()/(images.shape[-1]**2)\n    \n    train_loss /= len(train_loader.dataset)\n    train_accuracy /= len(train_loader.dataset)\n    return train_loss, train_accuracy\n\n\n@torch.no_grad()\ndef validation_epoch(model, criterion, val_loader, tqdm_desc):\n    val_loss, val_accuracy = 0.0, 0.0\n    tp, fn, tn, fp = 0.0, 0.0, 0.0, 0.0\n    \n    model.eval()\n    \n    for images, masks in tqdm(val_loader, desc=tqdm_desc):\n        images = images.to(device)\n        masks = masks.to(device)\n        \n        logits = model(images)['out']\n        loss = criterion(logits, masks)\n\n        val_loss += loss.item() * images.shape[0]\n        \n        labels = logits.argmax(dim=1)\n        labels_masks = (labels == masks)\n        labels_not_masks = torch.logical_not(labels_masks)\n        labels0 = (labels == 0)\n        labels1 = torch.logical_not(labels0)\n        \n        val_accuracy += labels_masks.sum().item()/(images.shape[-1]**2)\n        tp += torch.logical_and(labels_masks, labels1).sum().item()\n        fn += torch.logical_and(labels_not_masks, labels0).sum().item()\n        tn += torch.logical_and(labels_masks, labels0).sum().item()\n        fp += torch.logical_and(labels_not_masks, labels1).sum().item()        \n\n    val_loss /= len(val_loader.dataset)\n    \n    val_accuracy /= len(val_loader.dataset)\n    recall = tp / (tp + fn + 1e-16)        \n    precision = tp / (tp + fp + 1e-16)\n    q = tp / (tp + fn + fp + 1e-16)\n    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n \n    return val_loss, val_accuracy, recall, precision, q, f1\n\n    \ndef train(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs):\n    train_losses, train_accuracies = [], []\n    val_losses, val_accuracies = [], []\n    recalls, precisions, qs, f1s = [], [], [], []\n\n    for epoch in range(1, num_epochs + 1):\n        train_loss, train_accuracy = training_epoch(\n            model, optimizer, criterion, train_loader,\n            tqdm_desc=f'Training {epoch}/{num_epochs}'\n        )\n        val_loss, val_accuracy, recall, precision, q, f1 = validation_epoch(\n            model, criterion, val_loader,\n            tqdm_desc=f'Validating {epoch}/{num_epochs}'\n        )\n\n        if scheduler is not None:\n            scheduler.step()\n\n        train_losses += [train_loss]\n        train_accuracies += [train_accuracy]\n        val_losses += [val_loss]\n        val_accuracies += [val_accuracy]\n        recalls += [recall]\n        precisions += [precision]\n        qs += [q]\n        f1s += [f1]\n        plot_losses(train_losses, val_losses, train_accuracies, val_accuracies, recalls, precisions, qs, f1s)\n        \n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            }, '/kaggle/working/checkpoint')\n        \n    return train_losses, val_losses, train_accuracies, val_accuracies, recalls, precisions, qs, f1s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 30\nmodel = models.segmentation.fcn_resnet50(num_classes=2).to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)\ncriterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.5]).to(device))\nscheduler = None\n\ntrain_losses, val_losses, train_accuracies, val_accuracies, recalls, precisions, qs, f1s = train(\n    model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = models.segmentation.fcn_resnet50(num_classes=2).to(device)\n\n# checkpoint = torch.load('/kaggle/working/checkpoint')\n# model.load_state_dict(checkpoint['model_state_dict'])\n\n# model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef plot_results(dataset, n_examples):\n    \n    fig, ax = plt.subplots(n_examples, 3, figsize=(15, 5 * n_examples))\n    \n    model.eval()\n    \n    inds = np.random.choice(len(dataset), n_examples)\n    \n    for i in range(n_examples):\n        img, m = dataset[inds[i]]\n    \n        ax[i, 0].imshow((img * 0.5 + 0.5).permute(1, 2, 0))\n        ax[i, 0].set_title('train image №' + str(inds[i]))\n        \n        ax[i, 1].imshow(m, cmap='gray', vmin=0, vmax=1)\n        ax[i, 1].set_title('train ground truth №' + str(inds[i]))\n            \n        img = img.unsqueeze(0).to(device)\n        predicted_m = model(img)['out'].argmax(dim=1).squeeze().cpu()\n        \n        ax[i, 2].imshow(predicted_m, cmap='gray', vmin=0, vmax=1)\n        ax[i, 2].set_title('train predicted mask №' + str(inds[i]))\n    \n    for i in range(n_examples):\n        for j in range(3):            \n            ax[i, j].set_xticks([])\n            ax[i, j].set_yticks([])\n    \n    plt.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results(train_dataset, n_examples=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results(val_dataset, n_examples=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = ForestRoadsDataset('/kaggle/input/coursework/data/test/test_info.csv', \n                                   '/kaggle/input/coursework/data/test', \n                                   val_image_transforms, mask_transforms)\n\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, pin_memory=True, num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    tp, fn, tn, fp = 0, 0, 0, 0\n    \n    model.eval()\n    \n    for images, masks in test_loader:\n        images = images.to(device)\n        masks = masks.to(device)\n\n        logits = model(images)['out']\n        \n        labels = logits.argmax(dim=1)\n        labels_masks = (labels == masks)\n        labels_not_masks = torch.logical_not(labels_masks)\n        labels0 = (labels == 0)\n        labels1 = torch.logical_not(labels0)\n        \n        tp += torch.logical_and(labels_masks, labels1).sum().item()\n        fn += torch.logical_and(labels_not_masks, labels0).sum().item()\n        tn += torch.logical_and(labels_masks, labels0).sum().item()\n        fp += torch.logical_and(labels_not_masks, labels1).sum().item()        \n    \naccuracy = ((tp + tn) / (test_dataset[0][0].shape[-1]**2)) / len(test_loader.dataset)\nrecall = tp / (tp + fn + 1e-16)        \nprecision = tp / (tp + fp + 1e-16)\nq = tp / (tp + fn + fp + 1e-16)\nf1 = 2*tp / (2*tp + fn + fp + 1e-16)\n\nprint('accuracy: %0.5f' % accuracy)\nprint('recall: %0.5f' % recall)\nprint('precision: %0.5f' % precision)\nprint('iou: %0.5f' % q)\nprint('f1: %0.5f' % f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results(test_dataset, n_examples=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, '/kaggle/working/fcn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.load('/kaggle/working/fcn')\nmodel.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.ao.quantization import get_default_qconfig\nfrom torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\nfrom torch.ao.quantization import QConfigMapping\n\ndevice = torch.device('cpu')\nmodel = model.to(device)\nmodel.eval()\n\nqconfig = get_default_qconfig('qnnpack')\nqconfig_mapping = QConfigMapping().set_global(qconfig)\ndef calibrate(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        for image, target in data_loader:\n            model(image)\nexample_inputs = (next(iter(train_loader))[0]) \nprepared_model = prepare_fx(model, qconfig_mapping, example_inputs)  \ncalibrate(prepared_model, test_loader)  \nquantized_model = convert_fx(prepared_model)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quantized_model(test_dataset[0][0].unsqueeze(0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = quantized_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    tp, fn, tn, fp = 0, 0, 0, 0\n    \n    model.eval()\n    \n    for images, masks in test_loader:\n        images = images.to(device)\n        masks = masks.to(device)\n\n        logits = model(images)['out']\n        \n        labels = logits.argmax(dim=1)\n        labels_masks = (labels == masks)\n        labels_not_masks = torch.logical_not(labels_masks)\n        labels0 = (labels == 0)\n        labels1 = torch.logical_not(labels0)\n        \n        tp += torch.logical_and(labels_masks, labels1).sum().item()\n        fn += torch.logical_and(labels_not_masks, labels0).sum().item()\n        tn += torch.logical_and(labels_masks, labels0).sum().item()\n        fp += torch.logical_and(labels_not_masks, labels1).sum().item()        \n    \naccuracy = ((tp + tn) / (test_dataset[0][0].shape[-1]**2)) / len(test_loader.dataset)\nrecall = tp / (tp + fn + 1e-16)        \nprecision = tp / (tp + fp + 1e-16)\nq = tp / (tp + fn + fp + 1e-16)\nf1 = 2*tp / (2*tp + fn + fp + 1e-16)\n\nprint('accuracy: %0.5f' % accuracy)\nprint('recall: %0.5f' % recall)\nprint('precision: %0.5f' % precision)\nprint('iou: %0.5f' % q)\nprint('f1: %0.5f' % f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results(test_dataset, n_examples=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.mobile_optimizer import optimize_for_mobile\n\ndevice = torch.device('cpu')\nmodel = model.to(device)\nmodel.eval()\n\ndummy_input = torch.rand(1, 3, 512, 512).to(device)\n\ntorchscript_model = torch.jit.trace(model, dummy_input, strict=False)\noptimized_torchscript_model = optimize_for_mobile(torchscript_model)\noptimized_torchscript_model._save_for_lite_interpreter(\"optimized_torchscript_model_fcn_quant\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}